{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model to deploy\n",
    "We have decided to use RandomForestClassifier as the final model due to the impressive f1-score and because it is less expensive than XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tqdm \n",
    "import numpy as np \n",
    "\n",
    "import nltk \n",
    "import re\n",
    "import gensim\n",
    "\n",
    "import altair as alt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from praw.models import MoreComments\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "import skipthoughts\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>created</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Flair</th>\n",
       "      <th>new_feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['untouchability', 'even', 'quarantine', 'neve...</td>\n",
       "      <td>58</td>\n",
       "      <td>fzvwz8</td>\n",
       "      <td>['httpswwwtelegraphindiacomindiacoronavirusout...</td>\n",
       "      <td>6</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>1.586723e+09</td>\n",
       "      <td>['let', 'feel', 'hungry', 'couple', 'day', 'ma...</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>['untouchability', 'even', 'quarantine', 'neve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['delhi', 'govt', 'source', 'name', 'cm', 'arv...</td>\n",
       "      <td>304</td>\n",
       "      <td>f7ogd8</td>\n",
       "      <td>['httpstwittercomanistatus1231093900518932480s...</td>\n",
       "      <td>30</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>1.582380e+09</td>\n",
       "      <td>['beyond', 'petty', 'inclusion', 'delhi', 'gov...</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>['delhi', 'govt', 'source', 'name', 'cm', 'arv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>['delhi', 'ap', 'singh', 'advocate', '2012', '...</td>\n",
       "      <td>16</td>\n",
       "      <td>flgvah</td>\n",
       "      <td>['httpstwittercomanistatus1240731289075871744s...</td>\n",
       "      <td>19</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>1.584678e+09</td>\n",
       "      <td>['hunch', 'guy', 'trying', 'expose', 'loophole...</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>['delhi', 'ap', 'singh', 'advocate', '2012', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>['100', 'quota', 'tribal', 'teacher', 'school'...</td>\n",
       "      <td>18</td>\n",
       "      <td>g698qu</td>\n",
       "      <td>['httpswwwthehinducomnewsnationalno100quotafor...</td>\n",
       "      <td>2</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>1.587618e+09</td>\n",
       "      <td>['sc', 'point', '100', 'quota', 'ok', 'thats',...</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>['100', 'quota', 'tribal', 'teacher', 'school'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>['supreme', 'court', '’', 'verdict', 'scst', '...</td>\n",
       "      <td>105</td>\n",
       "      <td>f1o839</td>\n",
       "      <td>['httpsscrollinarticle952687whythesupremecourt...</td>\n",
       "      <td>47</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>1.581358e+09</td>\n",
       "      <td>['muslim', 'reservation', 'two', 'distraction'...</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>['supreme', 'court', '’', 'verdict', 'scst', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  \\\n",
       "0           0             0   \n",
       "1           1             1   \n",
       "2           2             2   \n",
       "3           3             3   \n",
       "4           4             4   \n",
       "\n",
       "                                               title  score      id  \\\n",
       "0  ['untouchability', 'even', 'quarantine', 'neve...     58  fzvwz8   \n",
       "1  ['delhi', 'govt', 'source', 'name', 'cm', 'arv...    304  f7ogd8   \n",
       "2  ['delhi', 'ap', 'singh', 'advocate', '2012', '...     16  flgvah   \n",
       "3  ['100', 'quota', 'tribal', 'teacher', 'school'...     18  g698qu   \n",
       "4  ['supreme', 'court', '’', 'verdict', 'scst', '...    105  f1o839   \n",
       "\n",
       "                                                 url  num_comments     body  \\\n",
       "0  ['httpswwwtelegraphindiacomindiacoronavirusout...             6  ['nan']   \n",
       "1  ['httpstwittercomanistatus1231093900518932480s...            30  ['nan']   \n",
       "2  ['httpstwittercomanistatus1240731289075871744s...            19  ['nan']   \n",
       "3  ['httpswwwthehinducomnewsnationalno100quotafor...             2  ['nan']   \n",
       "4  ['httpsscrollinarticle952687whythesupremecourt...            47  ['nan']   \n",
       "\n",
       "        created                                            Comment      Flair  \\\n",
       "0  1.586723e+09  ['let', 'feel', 'hungry', 'couple', 'day', 'ma...  Scheduled   \n",
       "1  1.582380e+09  ['beyond', 'petty', 'inclusion', 'delhi', 'gov...  Scheduled   \n",
       "2  1.584678e+09  ['hunch', 'guy', 'trying', 'expose', 'loophole...  Scheduled   \n",
       "3  1.587618e+09  ['sc', 'point', '100', 'quota', 'ok', 'thats',...  Scheduled   \n",
       "4  1.581358e+09  ['muslim', 'reservation', 'two', 'distraction'...  Scheduled   \n",
       "\n",
       "                                         new_feature  \n",
       "0  ['untouchability', 'even', 'quarantine', 'neve...  \n",
       "1  ['delhi', 'govt', 'source', 'name', 'cm', 'arv...  \n",
       "2  ['delhi', 'ap', 'singh', 'advocate', '2012', '...  \n",
       "3  ['100', 'quota', 'tribal', 'teacher', 'school'...  \n",
       "4  ['supreme', 'court', '’', 'verdict', 'scst', '...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_india = pd.read_csv('clean_reddit_india.csv')\n",
    "df_india.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Unnamed: 0.1', 'title', 'score', 'id', 'url',\n",
      "       'num_comments', 'body', 'created', 'Comment', 'Flair', 'new_feature'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_india.columns)\n",
    "df_india.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df_india = df_india.rename(columns={'Unnamed: 0.1':'Index'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AskIndia': 0, 'Business/Finance': 1, 'CAA-NRC-NPR': 2, 'Coronavirus': 3, 'Food': 4, 'Non-Political': 5, 'Photography': 6, 'Politics': 7, 'Scheduled': 8, 'Science/Technology': 9, 'Sports': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "df_india['Flair_cat'] = le.fit_transform(df_india['Flair'])\n",
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "print(le_name_mapping)\n",
    "with open('pickles/le_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(le_name_mapping, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', \n",
    "        encoding='latin-1', ngram_range=(1, 2), stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2107, 3830)\n"
     ]
    }
   ],
   "source": [
    "features_train = tfidf.fit_transform(df_india['new_feature']).toarray()\n",
    "labels_train = df_india['Flair_cat']\n",
    "print(features_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      " 'AskIndia':\n",
      "Most correlated unigrams: \n",
      "flat\n",
      "subreddit\n",
      "rid\n",
      "aunty\n",
      "askindia\n",
      "Most correlated bigrams: \n",
      "doesnt matter\n",
      "spend time\n",
      "dont use\n",
      "ask india\n",
      "askindia need\n",
      "1\n",
      " 'Business/Finance':\n",
      "Most correlated unigrams: \n",
      "week\n",
      "2016\n",
      "ending\n",
      "business\n",
      "finance\n",
      "Most correlated bigrams: \n",
      "indian startup\n",
      "indian economy\n",
      "week ending\n",
      "economy week\n",
      "week indian\n",
      "2\n",
      " 'CAA-NRC-NPR':\n",
      "Most correlated unigrams: \n",
      "caanprnrc\n",
      "protest\n",
      "caa\n",
      "nrc\n",
      "npr\n",
      "Most correlated bigrams: \n",
      "join protest\n",
      "npr nrc\n",
      "caa nrc\n",
      "caa npr\n",
      "nrc npr\n",
      "3\n",
      " 'Coronavirus':\n",
      "Most correlated unigrams: \n",
      "case\n",
      "lockdown\n",
      "covid19\n",
      "testing\n",
      "coronavirus\n",
      "Most correlated bigrams: \n",
      "coronavirus nan\n",
      "coronavirus pandemic\n",
      "india coronavirus\n",
      "coronavirus lockdown\n",
      "coronavirus case\n",
      "4\n",
      " 'Food':\n",
      "Most correlated unigrams: \n",
      "migrant\n",
      "eat\n",
      "lockdown\n",
      "delivery\n",
      "food\n",
      "Most correlated bigrams: \n",
      "food supply\n",
      "provide food\n",
      "free food\n",
      "indian food\n",
      "food delivery\n",
      "5\n",
      " 'Non-Political':\n",
      "Most correlated unigrams: \n",
      "rindia\n",
      "celebrating\n",
      "political\n",
      "non\n",
      "nonpolitical\n",
      "Most correlated bigrams: \n",
      "right thing\n",
      "year old\n",
      "im bot\n",
      "nonpolitical nan\n",
      "non political\n",
      "6\n",
      " 'Photography':\n",
      "Most correlated unigrams: \n",
      "photographer\n",
      "camera\n",
      "thread\n",
      "weekly\n",
      "photography\n",
      "Most correlated bigrams: \n",
      "2019 nan\n",
      "thread september\n",
      "thread thread\n",
      "weekly photography\n",
      "photography thread\n",
      "7\n",
      " 'Politics':\n",
      "Most correlated unigrams: \n",
      "scindia\n",
      "party\n",
      "bjp\n",
      "political\n",
      "politics\n",
      "Most correlated bigrams: \n",
      "rahul gandhi\n",
      "indian politics\n",
      "political party\n",
      "current political\n",
      "politics nan\n",
      "8\n",
      " 'Scheduled':\n",
      "Most correlated unigrams: \n",
      "airport\n",
      "poll\n",
      "caste\n",
      "schedule\n",
      "scheduled\n",
      "Most correlated bigrams: \n",
      "random daily\n",
      "scheduled tribe\n",
      "daily discussion\n",
      "game thread\n",
      "scheduled caste\n",
      "9\n",
      " 'Science/Technology':\n",
      "Most correlated unigrams: \n",
      "research\n",
      "innovation\n",
      "scientific\n",
      "technology\n",
      "science\n",
      "Most correlated bigrams: \n",
      "data science\n",
      "department science\n",
      "technology innovation\n",
      "indian science\n",
      "science technology\n",
      "10\n",
      " 'Sports':\n",
      "Most correlated unigrams: \n",
      "cricket\n",
      "league\n",
      "ipl\n",
      "football\n",
      "sport\n",
      "Most correlated bigrams: \n",
      "unpopular opinion\n",
      "authority india\n",
      "sport india\n",
      "world cup\n",
      "star sport\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "\n",
    "column_values = df_india['Flair_cat'].values\n",
    "flair_list = pd.unique(column_values)\n",
    "\n",
    "N = 5\n",
    "for flair_cat in sorted(flair_list):\n",
    "    print(flair_cat)\n",
    "    features_chi2 = chi2(features_train, labels_train==flair_cat)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [word for word in feature_names if len(word.split(' '))==1]\n",
    "    bigrams = [word for word in feature_names if len(word.split(' '))==2]\n",
    "    flair = df_india.loc[lambda df_india:df_india['Flair_cat']==flair_cat]\n",
    "    print(\" '{}':\".format(flair['Flair'].iloc[0]))\n",
    "    print(\"Most correlated unigrams: \\n{}\".format('\\n'.join(unigrams[-N:])))\n",
    "    print(\"Most correlated bigrams: \\n{}\".format('\\n'.join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
    "                       criterion='gini', max_depth=70, max_features='sqrt',\n",
    "                       max_leaf_nodes=None, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=3, min_samples_split=10,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=1100,\n",
    "                       n_jobs=None, oob_score=False, random_state=8, verbose=0,\n",
    "                       warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=70, max_features='sqrt',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=3, min_samples_split=10,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1100,\n",
       "                       n_jobs=None, oob_score=False, random_state=8, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is: \n",
      "0.912672045562411\n"
     ]
    }
   ],
   "source": [
    "print(\"The training accuracy is: \")\n",
    "print(accuracy_score(labels_train, estimator.predict(features_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles_deploy/rfc_estimator.pkl', 'wb') as f:\n",
    "    pickle.dump(estimator, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles_deploy/tfidf.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('gpu': conda)",
   "language": "python",
   "name": "python361064bitgpucondae32c6e92795749dd8636718cf39723f4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
