{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to scrape and collect data.\n",
    "\n",
    "\n",
    "Python Reddit API Wrapper(PRAW) to be used to collect data.<br>\n",
    "Documentation: https://praw.readthedocs.io/en/latest/ <br>\n",
    "We will collect 200 data instances of each flair.*<br>\n",
    "\n",
    "The flairs we classify are the ones on  reddit as per 7th April 2020 <br>\n",
    "1) Scheduled <br>\n",
    "2) Politics <br>\n",
    "3) Photography <br>\n",
    "4) AskIndia <br>\n",
    "5) Sports <br>\n",
    "6) Non-Political <br>\n",
    "7) Science/Technology <br>\n",
    "8) Food <br>\n",
    "9) Business/Finance <br>\n",
    "10) Coronavirus<br>\n",
    "11) CAA-NRC-NPR <br>\n",
    "*flair may be subject to changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import nltk\n",
    "import re\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from praw.models import MoreComments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 200 posts are chosen along with necessart attributes.\n",
    "For each post we extract the top level comments.\n",
    "We choose only the top level to make sure that relevance is maintained.\n",
    "The dataset itself is written to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  title  score      id  \\\n",
      "0     Untouchability, even in quarantine. 'We have n...     58  fzvwz8   \n",
      "1     Delhi Govt Sources: Names of CM Arvind Kejriwa...    304  f7ogd8   \n",
      "2     Delhi: AP Singh, advocate of 2012 Delhi gang-r...     16  flgvah   \n",
      "3     No 100% quota for tribal teachers in schools l...     18  g698qu   \n",
      "4     Why the Supreme Court’s verdict on SC/ST quota...    105  f1o839   \n",
      "...                                                 ...    ...     ...   \n",
      "2102  The state if suppression and labeling those in...      2  evi93u   \n",
      "2103  Why can't we protest for relections? They brok...     16  efrzjb   \n",
      "2104   Something to reflect on during ongoing protests.     30  el6n9o   \n",
      "2105  Unofficial termination letter by Students of J...     94  ef0n1x   \n",
      "2106  Citizenship Amendment Act(CAB/CAA), National R...     14  ec4rbh   \n",
      "\n",
      "                                                    url  num_comments  \\\n",
      "0     https://www.telegraphindia.com/india/coronavir...             6   \n",
      "1     https://twitter.com/ANI/status/123109390051893...            30   \n",
      "2     https://twitter.com/ani/status/124073128907587...            19   \n",
      "3     https://www.thehindu.com/news/national/no-100-...             2   \n",
      "4     https://scroll.in/article/952687/why-the-supre...            47   \n",
      "...                                                 ...           ...   \n",
      "2102  https://www.reddit.com/r/india/comments/evi93u...             2   \n",
      "2103  https://www.reddit.com/r/india/comments/efrzjb...             5   \n",
      "2104  https://www.reddit.com/r/india/comments/el6n9o...             2   \n",
      "2105  https://www.reddit.com/r/india/comments/ef0n1x...             0   \n",
      "2106  https://www.reddit.com/r/india/comments/ec4rbh...             2   \n",
      "\n",
      "                                                   body       created  \\\n",
      "0                                                        1.586723e+09   \n",
      "1                                                        1.582380e+09   \n",
      "2                                                        1.584678e+09   \n",
      "3                                                        1.587618e+09   \n",
      "4                                                        1.581358e+09   \n",
      "...                                                 ...           ...   \n",
      "2102  \\n\\nIndia above everything is a democracy. But...  1.580303e+09   \n",
      "2103  People should just protest for all the NRC NPR...  1.577371e+09   \n",
      "2104  With the ongoing protests over the CAA and the...  1.578402e+09   \n",
      "2105  [Stop Convocation](https://www.ndtv.com/india-...  1.577219e+09   \n",
      "2106  The internet rabbit hole took me to various pl...  1.576658e+09   \n",
      "\n",
      "                                                Comment        Flair  \n",
      "0     Let them feel hungry for a couple of days, max...    Scheduled  \n",
      "1     This is beyond petty.> The inclusion of a Delh...    Scheduled  \n",
      "2     My hunch is , this guy is trying to expose the...    Scheduled  \n",
      "3     When SC has to point out that 100% quota in no...    Scheduled  \n",
      "4     Muslims and reservation are two distractions u...    Scheduled  \n",
      "...                                                 ...          ...  \n",
      "2102  A lot of the blame for this state of affairs m...  CAA-NRC-NPR  \n",
      "2103  The constitutional way of re-election is mobil...  CAA-NRC-NPR  \n",
      "2104  The protests are a symbol of, among others, a ...  CAA-NRC-NPR  \n",
      "2105                                                     CAA-NRC-NPR  \n",
      "2106  Thank you for making this post. It is true tha...  CAA-NRC-NPR  \n",
      "\n",
      "[2107 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "flairs = [\"Scheduled\", \"Politics\", \"Photography\", \"AskIndia\", \"Sports\",\n",
    "        \"Non-Political\", \"Science/Technology\", \"Food\", \"Business/Finance\",\n",
    "        \"Coronavirus\", \"CAA-NRC-NPR\"]\n",
    "\n",
    "\n",
    "reddit = praw.Reddit(client_id='7hx8xkseiQkWNA', client_secret='UPd0tXn-Lt26mk4-Z38oCNLuG_8', user_agent='Kevin Stephen')\n",
    "india = reddit.subreddit('india')\n",
    "\n",
    "results = []\n",
    "\n",
    "for flair in flairs:\n",
    "    posts = india.search(flair, limit=200)\n",
    "\n",
    "    for post in posts:\n",
    "        comment = ''\n",
    "        post.comment_sort = 'best'\n",
    "        post.comment_limit = 3\n",
    "        for top_level_comment in post.comments:\n",
    "            if isinstance(top_level_comment, MoreComments):\n",
    "                continue\n",
    "            comment += top_level_comment.body\n",
    "        results.append([post.title, post.score, post.id, post.url, post.num_comments, post.selftext, post.created, comment, flair])\n",
    "\n",
    "results = pd.DataFrame(results ,columns=['title', 'score', 'id', 'url', 'num_comments', 'body', 'created', 'Comment', 'Flair'])\n",
    "print(results)\n",
    "results.to_csv('reddit_india_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Let them feel hungry for a couple of days, max...\n",
       "1     This is beyond petty.> The inclusion of a Delh...\n",
       "2     My hunch is , this guy is trying to expose the...\n",
       "3     When SC has to point out that 100% quota in no...\n",
       "4     Muslims and reservation are two distractions u...\n",
       "5     Bachega India, tabhi toh Padhega India.Gand ma...\n",
       "6     Oh boy. Chalo bhaisahab. Sabji ka dukaan main ...\n",
       "7     AFAIK, U.S. still hasn't banned entries of Ind...\n",
       "8     But we are funding Sanskrit Universities with ...\n",
       "9     Someone give me a ELI5? What's this drug used ...\n",
       "10    Yes I think you are legally eligible to claim ...\n",
       "11    Bc students in 12 and 10 are not immune to the...\n",
       "12                                                  NaN\n",
       "13    Hey man just sleep already, maintain a sleep h...\n",
       "14    Unless you are in a dire need of job, skip the...\n",
       "15    Our society is going back to the rotten eras o...\n",
       "16    The single biggest minus point about Indian so...\n",
       "17    Bangladesh is literally the only friendly neig...\n",
       "18    This has happened to me. I got suspicious abou...\n",
       "19    My state, Manipur might have perrenial conflic...\n",
       "Name: Comment, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_india = pd.read_csv('reddit_india_test.csv')\n",
    "df_india['Comment'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string \n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating any feature from the raw text, we must perform a cleaning process\n",
    "to ensure no distortions are introduced to the model. The following steps have been followed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special character cleaning: special characters such as “\\n” double quotes\n",
    "must be removed from the text since we aren’t expecting any predicting\n",
    "power from them.\n",
    "\n",
    "Upcase/downcase: we would expect, for example, “Book” and “book” to be\n",
    "the same word and have the same predicting power. For that reason we\n",
    "have downcased every word.\n",
    "\n",
    "Punctuation signs: characters such as “?”, “!”, “;” have been removed.\n",
    "\n",
    "Possessive pronouns: in addition, we would expect that “Trump” and\n",
    "“Trump’s” had the same predicting power.\n",
    "\n",
    "Stemming or Lemmatization: stemming is the process of reducing\n",
    "derived words to their root. Lemmatization is the process of reducing a word\n",
    "to its lemma. The main difference between both methods is that\n",
    "lemmatization provides existing words, whereas stemming provides the\n",
    "root, which may not be an existing word. We have used a Lemmatizer based\n",
    "in WordNet. I have preferred lemmatizing over stemming because it is more accurate. eg. chance may become chanc during stemming but remains chance during lemmatization\n",
    "\n",
    "Stop words: words such as “what” or “the” won’t have any predicting power\n",
    "since they will presumably be common to all the documents. For this reason,\n",
    "they may represent noise that can be eliminated. We have downloaded a list\n",
    "of English stop words from the nltk package and then deleted them from the\n",
    "corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_string(text):\n",
    "    return str(text)\n",
    "\n",
    "def remove_punc(text):\n",
    "    text_nopunc = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunc\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    return text\n",
    "\n",
    "def lemmatizer(text):\n",
    "    wn = nltk.WordNetLemmatizer()\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_india['title'] = df_india['title'].apply(lambda x: ret_string(x))\n",
    "df_india['title'] = df_india['title'].apply(lambda x: remove_punc(x))\n",
    "df_india['title'] = df_india['title'].apply(lambda x: tokenize(x))\n",
    "df_india['title'] = df_india['title'].apply(lambda x: remove_stopwords(x))\n",
    "df_india['title'] = df_india['title'].apply(lambda x: lemmatizer(x))\n",
    "\n",
    "df_india['Comment'] = df_india['Comment'].apply(lambda x: ret_string(x))\n",
    "df_india['Comment'] = df_india['Comment'].apply(lambda x: remove_punc(x))\n",
    "df_india['Comment'] = df_india['Comment'].apply(lambda x: tokenize(x))\n",
    "df_india['Comment'] = df_india['Comment'].apply(lambda x: remove_stopwords(x))\n",
    "df_india['Comment'] = df_india['Comment'].apply(lambda x: lemmatizer(x))\n",
    "\n",
    "df_india['body'] = df_india['body'].apply(lambda x: ret_string(x))\n",
    "df_india['body'] = df_india['body'].apply(lambda x: remove_punc(x))\n",
    "df_india['body'] = df_india['body'].apply(lambda x: tokenize(x))\n",
    "df_india['body'] = df_india['body'].apply(lambda x: remove_stopwords(x))\n",
    "df_india['body'] = df_india['body'].apply(lambda x: lemmatizer(x))\n",
    "\n",
    "df_india['url'] = df_india['url'].apply(lambda x: ret_string(x))\n",
    "df_india['url'] = df_india['url'].apply(lambda x: remove_punc(x))\n",
    "df_india['url'] = df_india['url'].apply(lambda x: tokenize(x))\n",
    "df_india['url'] = df_india['url'].apply(lambda x: remove_stopwords(x))\n",
    "df_india['url'] = df_india['url'].apply(lambda x: lemmatizer(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new feature\n",
    "We empirically find that the best results are obtained by combining title and the top level comments <br>\n",
    "I have chosen to leave out body as a feature due to the fact that the length is highly variable and may contain irrelevant terms as these are penned by thousands of people in the country <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [untouchability, even, quarantine, never, take...\n",
       "1     [delhi, govt, source, name, cm, arvind, kejriw...\n",
       "2     [delhi, ap, singh, advocate, 2012, delhi, gang...\n",
       "3     [100, quota, tribal, teacher, school, located,...\n",
       "4     [supreme, court, ’, verdict, scst, quota, crea...\n",
       "5     [entrance, exam, scheduled, may, bachega, indi...\n",
       "6     [advisory, scheduled, international, commercia...\n",
       "7     [roommate, india, he, indian, american, citize...\n",
       "8     [ministry, score, 100, fund, utilisation, sche...\n",
       "9     [hydroxychloroquine, schedule, h1, drug, sold,...\n",
       "10    [askindia, indigo, cancelled, flight, le, 8, h...\n",
       "11    [maharashtra, govt, school, urban, area, mahar...\n",
       "12    [nirbhaya, case, sc, quashes, death, row, conv...\n",
       "13    [let, fix, sleep, schedule, hey, man, sleep, a...\n",
       "14    [hr, people, people, ladder, applied, role, tr...\n",
       "15    [massive, mob, storm, scheduled, caste, colony...\n",
       "16    [india, highly, questionable, work, culture, b...\n",
       "17    [source, bangladesh, foreign, minister, ak, ab...\n",
       "18    [job, consultancy, company, request, pan, card...\n",
       "19    [continuing, practice, untouchability, india, ...\n",
       "Name: new_feature, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_india['new_feature'] = df_india['title']+df_india['Comment']\n",
    "df_india['new_feature'].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_india.to_csv('clean_reddit_india.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('gpu': conda)",
   "language": "python",
   "name": "python361064bitgpucondae32c6e92795749dd8636718cf39723f4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
